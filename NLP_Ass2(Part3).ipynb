{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Ass2(Part3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRz8-cNqH47N",
        "colab_type": "text"
      },
      "source": [
        "**Xing Yi Chan**\n",
        "\n",
        "**R00183768**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1-t4FIvH9M2",
        "colab_type": "text"
      },
      "source": [
        "### **Part 3**\n",
        "\n",
        "The third task asks machine to generate the reasons and we use BLEU to evaluate them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl7MjG7HIB-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "from pickle import dump, load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lekedls0IIRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import both training and testing data\n",
        "train_data = pd.read_csv('/content/drive/My Drive/NLP/dataset2/traindata/subtaskC_data_all.csv').values\n",
        "train_label = pd.read_csv('/content/drive/My Drive/NLP/dataset2/traindata/subtaskC_answers_all.csv', names=['id', 's1', 's2', 's3']).values\n",
        "\n",
        "test_data = pd.read_csv('/content/drive/My Drive/NLP/dataset2/testdata/subtaskC_trial_data.csv').values\n",
        "test_label = pd.read_csv('/content/drive/My Drive/NLP/dataset2/testdata/subtaskC_answers.csv').values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcHrmcSCOR8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preparing text\n",
        "text = ' '\n",
        "for x in range(len(train_data)):\n",
        "    # combine all sentences in train_data and train_label\n",
        "    sentences = train_data[x][1] + train_label[x][1] + train_label[x][2] + train_label[x][3]\n",
        "    sentences = sentences.replace('.', ' ').lower() # replace fullstops to spaces and convert every word to lower case\n",
        "    text = text + sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foPU6mqeTpIU",
        "colab_type": "text"
      },
      "source": [
        "### **Clean and save text**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF4fLqUZTwND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "\tdoc = doc.replace('--', ' ')\n",
        "\t# split into tokens by white space\n",
        "\ttokens = doc.split()\n",
        "\t# remove punctuation from each token\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\t# remove remaining tokens that are not alphabetic\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\treturn tokens\n",
        "\n",
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "tokens = clean_doc(text)\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "\t# select sequence of tokens\n",
        "\tseq = tokens[i-length:i]\n",
        "\t# convert into a line\n",
        "\tline = ' '.join(seq)\n",
        "\t# store\n",
        "\tsequences.append(line)\n",
        "\n",
        "# write sequences into a file\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    datafile = open(filename, 'w')\n",
        "    datafile.write(data)\n",
        "    datafile.close()\n",
        "\n",
        "filename ='/content/drive/My Drive/NLP/dataset2/Part3_data'\n",
        "save_doc(sequences, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyyMr8lLb1Fv",
        "colab_type": "text"
      },
      "source": [
        "### **Read and encode sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8SvNdCRb7Lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load sequences\n",
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load file\n",
        "doc = load_doc(filename)\n",
        "lines = doc.split('\\n')\n",
        "lines = lines[:150000]\n",
        "\n",
        "# encode sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "# vocab size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# seperate input and output\n",
        "sequences = np.array(sequences)\n",
        "\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr0yHwZ6eMkN",
        "colab_type": "text"
      },
      "source": [
        "### **Create and train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxPTkJLEeS9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "7439acec-4185-407c-85d3-ef6bc1bd4cab"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 50, 50)            773200    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15464)             1561864   \n",
            "=================================================================\n",
            "Total params: 2,485,964\n",
            "Trainable params: 2,485,964\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiXkj1pi0fmz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "e04e685b-88a9-463e-e244-5d49104816d4"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "150000/150000 [==============================] - 208s 1ms/step - loss: 6.7295 - accuracy: 0.0659\n",
            "Epoch 2/5\n",
            "150000/150000 [==============================] - 199s 1ms/step - loss: 6.1733 - accuracy: 0.1176\n",
            "Epoch 3/5\n",
            "150000/150000 [==============================] - 200s 1ms/step - loss: 5.8137 - accuracy: 0.1533\n",
            "Epoch 4/5\n",
            "150000/150000 [==============================] - 200s 1ms/step - loss: 5.5246 - accuracy: 0.1771\n",
            "Epoch 5/5\n",
            "150000/150000 [==============================] - 200s 1ms/step - loss: 5.2971 - accuracy: 0.1922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5a7aed1b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S0zvMhF1cuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model to file\n",
        "model.save('/content/drive/My Drive/NLP/dataset2/model.h5')\n",
        "\n",
        "# save tokenizer\n",
        "dump(tokenizer, open('/content/drive/My Drive/NLP/dataset2/tokenizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLoq4rza2JDT",
        "colab_type": "text"
      },
      "source": [
        "### **Generate text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w36HENyWyQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate sequence using language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "\n",
        "    # generate a fixed number of words:\n",
        "    for i in range(n_words):\n",
        "        # encode text into integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        prob = model.predict_classes(encoded, verbose=0)\n",
        "        # map predicted index word to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == prob:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append into input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0yXBSnE5wTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "6ca2bb7d-2fa5-42c1-ed2a-d987e2793db7"
      },
      "source": [
        "# load the model\n",
        "model = load_model('/content/drive/My Drive/NLP/dataset2/model.h5')\n",
        " \n",
        "# load the tokenizer\n",
        "tokenizer = load(open('/content/drive/My Drive/NLP/dataset2/tokenizer.pkl', 'rb'))\n",
        " \n",
        "# generate new text\n",
        "results = []\n",
        "for test_item in test_data:\n",
        "    result = []\n",
        "    i = test_item[0]\n",
        "    sentence = test_item[1]\n",
        "    response = generate_seq(model, tokenizer, seq_length, sentence, 10)\n",
        "    result.extend([i, response])\n",
        "    results.append(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-4xOHeF7Vg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save generated results into file\n",
        "pd.DataFrame(results).to_csv('/content/drive/My Drive/NLP/PartC(results).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5tiXl51WxM1",
        "colab_type": "text"
      },
      "source": [
        "### **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRYuRtBMW02_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa3ce72a-9ecf-4fdc-e7c4-cc7ae09c00a9"
      },
      "source": [
        "# calculate the bleu score of the generated text\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# clean the sentences\n",
        "def clean_sen(input_sen):\n",
        "    # split into tokens by white space\n",
        "    tokens = input_sen.split()\n",
        "\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "hypothesis = clean_sen(test_label)\n",
        "reference = clean_sen(results)\n",
        "\n",
        "bleuscore = sentence_bleu(reference, hypothesis, weights = (0.5, 0.5))\n",
        "print('Bleu score of generated text is :', bleuscore)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bleu score of generated text is : 71.63\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}