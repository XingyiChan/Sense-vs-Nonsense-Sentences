{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Ass2(Part1).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1Xzl0_sxzl6wB-ue4jVqUzUDrz1f-WvJm","authorship_tag":"ABX9TyOJD62e2jWo5wARcQvzcT3i"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ste17EeB8lG4","colab_type":"text"},"source":["**Xing Yi Chan**\n","\n","**R00183768**"]},{"cell_type":"markdown","metadata":{"id":"XUPuSehD83pK","colab_type":"text"},"source":["## **Part 1**\n","The first task is to choose from two natural language statements with similar wordings which one makes sense and which one does not make sense."]},{"cell_type":"code","metadata":{"id":"lMO6mgJq031d","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-AUPM7AB3Wj","colab_type":"text"},"source":["### **Preparing Training Data**"]},{"cell_type":"code","metadata":{"id":"Kd76vUULCnBI","colab_type":"code","colab":{}},"source":["# import both training data and training labels\n","train_data = pd.read_csv('/content/drive/My Drive/NLP/dataset2/traindata/subtaskA_data_all.csv')\n","train_label = pd.read_csv('/content/drive/My Drive/NLP/dataset2/traindata/subtaskA_answers_all.csv', names=['id', 'ans'])\n","\n","# extract positive sentences\n","train_pos1 = train_data[train_label['ans'] == 0]['sent1']\n","train_pos2 = train_data[train_label['ans'] == 1]['sent0']\n","train_pos_sent = train_pos1.append(train_pos2)\n","\n","# extract negative sentences\n","train_neg1 = train_data[train_label['ans'] == 0]['sent0']\n","train_neg2 = train_data[train_label['ans'] == 1]['sent1']\n","train_neg_sent = train_neg1.append(train_neg2)\n","\n","#combining both positive and negative sentences\n","   # sensible sentences --> 0\n","   # nonsense sentences --> 1\n","trainX = train_pos_sent.append(train_neg_sent)\n","# create labels for the sentences\n","trainY = pd.Series([0]*10000).append(pd.Series([1]*10000))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GSFu5QIxk28f","colab_type":"text"},"source":["### **Preparing Testing Data**"]},{"cell_type":"code","metadata":{"id":"DMeJ22s8lnjZ","colab_type":"code","colab":{}},"source":["# import both training data and training labels\n","test_data = pd.read_csv('/content/drive/My Drive/NLP/dataset2/testdata/subtaskA_trial_data.csv')\n","test_label = pd.read_csv('/content/drive/My Drive/NLP/dataset2/testdata/subtaskA_answers.csv', names=['id', 'ans'])\n","\n","# extract positive sentences\n","test_pos1 = test_data[test_label['ans'] == 0]['sent1']\n","test_pos2 = test_data[test_label['ans'] == 1]['sent0']\n","train_pos_sent = test_pos1.append(test_pos2)\n","\n","# extract negative sentences\n","test_neg1 = test_data[test_label['ans'] == 0]['sent0']\n","test_neg2 = test_data[test_label['ans'] == 1]['sent1']\n","test_neg_sent = test_neg1.append(test_neg2)\n","\n","#combining both positive and negative sentences\n","   # sensible sentences --> 0\n","   # nonsense sentences --> 1\n","testX = train_pos_sent.append(test_neg_sent)\n","# create labels for the sentences\n","testY = pd.Series([0]*2021).append(pd.Series([1]*2021))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k3LtrzJdn8dW","colab_type":"text"},"source":["## **Create tf-idf vectors for both training and testing data**"]},{"cell_type":"code","metadata":{"id":"QzxcMfusoTIi","colab_type":"code","colab":{}},"source":["# transform words to integers using TfidfVectorizer() function\n","vectorizer = TfidfVectorizer(max_features=2000, analyzer='word', ngram_range=(1, 4))\n","trainX_tfidf = vectorizer.fit_transform(trainX)\n","testX_tfidf = vectorizer.fit_transform(testX)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cvu7dedUqZDR","colab_type":"text"},"source":["## **Performs classification to get accuracy score**"]},{"cell_type":"code","metadata":{"id":"g0RJ4Ys8qujE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1595455422941,"user_tz":-60,"elapsed":110503,"user":{"displayName":"Xingyi Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNYWYUtCqJeuV1dkrvw6FkebdtDzj5ZkHCPlSTRA=s64","userId":"01867124944861915148"}},"outputId":"11cb4301-6bfa-488c-fcdd-8764507029f5"},"source":["# naive bayes classifier\n","nb = MultinomialNB()\n","nb.fit(trainX_tfidf, trainY)\n","print('Accuracy of naive bayes: ', nb.score(testX_tfidf, testY)*100)\n","\n","# k-nearest neighbour classifier     \n","knn = KNeighborsClassifier()\n","knn.fit(trainX_tfidf, trainY)\n","print('Accuracy of k-nn: ', knn.score(testX_tfidf, testY)*100)\n","\n","# random forest classifier\n","rf = RandomForestClassifier()\n","rf.fit(trainX_tfidf, trainY)\n","print('Accuracy of random forest: ', rf.score(testX_tfidf, testY)*100)\n","\n","# svm classifier\n","svm = SVC()\n","svm.fit(trainX_tfidf, trainY)\n","print('Accuracy of svm: ', svm.score(testX_tfidf, testY)*100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of naive bayes:  51.53389411182583\n","Accuracy of k-nn:  48.98565066798615\n","Accuracy of random forest:  50.643245917862444\n","Accuracy of svm:  50.569025235032164\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"39x7Xg8nh7AW","colab_type":"text"},"source":["### **Conclusion**"]},{"cell_type":"markdown","metadata":{"id":"IBZiTJtrgJfJ","colab_type":"text"},"source":["For classification, 4 different type of classification methods had been used. They include Multinomial Naive Bayes Classifier, k-Nearest Neighbour Classifier, Random Forest Classifier and Support Vector Machine Classifier. After comparing the accuracy of these classifiers, it can be concluded that all these classifiers will give an accuracy score between 50%. \n","\n","Among all the 4 classifiers, Multinomial Naive Bayes classifier performs best with the accuracy of 51.53%."]}]}