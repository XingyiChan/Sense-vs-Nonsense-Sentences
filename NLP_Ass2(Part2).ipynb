{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Ass2(Part2).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1SaPHlYBSNphbeq4T10beyn2i9cuxTzZy","authorship_tag":"ABX9TyMmxZzT+Tthj1xSZbCVQOr8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jbFmztkf_LnJ","colab_type":"text"},"source":["**Xing Yi Chan**\n","\n","**R00183768**"]},{"cell_type":"markdown","metadata":{"id":"BUZr2BT7_NCv","colab_type":"text"},"source":["## **Part 2**\n","\n","Select the most corresponding reason why this statement is against common sense."]},{"cell_type":"code","metadata":{"id":"yAJY3j99_bk8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595518561074,"user_tz":-60,"elapsed":1611,"user":{"displayName":"Xingyi Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNYWYUtCqJeuV1dkrvw6FkebdtDzj5ZkHCPlSTRA=s64","userId":"01867124944861915148"}}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ckfvuj09We6q","colab_type":"text"},"source":["### **Preparing Training Data**"]},{"cell_type":"code","metadata":{"id":"kI2UvDG5W4y5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595518566306,"user_tz":-60,"elapsed":6780,"user":{"displayName":"Xingyi Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNYWYUtCqJeuV1dkrvw6FkebdtDzj5ZkHCPlSTRA=s64","userId":"01867124944861915148"}}},"source":["train_data = pd.read_csv('/content/drive/My Drive/NLP/dataset2/traindata/subtaskB_data_all.csv')\n","train_label = pd.read_csv('/content/drive/My Drive/NLP/dataset2/traindata/subtaskB_answers_all.csv', names=['id', 'ans'])\n","\n","# extract positive sentences\n","train_pos_sent = train_data[train_label['ans'] == 'A']['OptionA']\n","train_pos_sent = train_pos_sent.append(train_data[train_label['ans'] == 'B']['OptionB'])\n","train_pos_sent = train_pos_sent.append(train_data[train_label['ans'] == 'C']['OptionC'])\n","\n","# extract negative sentences\n","train_neg_sent = train_data[train_label['ans'] == 'A']['OptionB']\n","train_neg_sent = train_neg_sent.append(train_data[train_label['ans'] == 'A']['OptionC'])\n","train_neg_sent = train_neg_sent.append(train_data[train_label['ans'] == 'B']['OptionA'])\n","train_neg_sent = train_neg_sent.append(train_data[train_label['ans'] == 'B']['OptionC'])\n","train_neg_sent = train_neg_sent.append(train_data[train_label['ans'] == 'C']['OptionA'])\n","train_neg_sent = train_neg_sent.append(train_data[train_label['ans'] == 'C']['OptionB'])\n","\n","#combining both positive and negative sentences\n","   # sensible sentences --> 0\n","   # nonsense sentences --> 1\n","trainX = train_pos_sent.append(train_neg_sent)\n","# create labels for the sentences\n","trainY = pd.Series([0]*10000).append(pd.Series([1]*20000))\n","\n","# reset index and avoid old index being added mistakenly as a column\n","trainX = trainX.reset_index(drop=True)\n","trainY = trainY.reset_index(drop=True)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PQlOePnnbgSR","colab_type":"text"},"source":["### **Preparing Testing Data**"]},{"cell_type":"code","metadata":{"id":"q2hegHaPWkwq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595518568628,"user_tz":-60,"elapsed":9079,"user":{"displayName":"Xingyi Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNYWYUtCqJeuV1dkrvw6FkebdtDzj5ZkHCPlSTRA=s64","userId":"01867124944861915148"}}},"source":["test_data = pd.read_csv('/content/drive/My Drive/NLP/dataset2/testdata/subtaskB_trial_data.csv')\n","test_label = pd.read_csv('/content/drive/My Drive/NLP/dataset2/testdata/subtaskB_answers.csv', names=['id', 'ans'])\n","\n","# extract positive sentences\n","test_pos_sent = test_data[test_label['ans'] == 'A']['OptionA']\n","test_pos_sent = test_pos_sent.append(test_data[test_label['ans'] == 'B']['OptionB'])\n","test_pos_sent = test_pos_sent.append(test_data[test_label['ans'] == 'C']['OptionC'])\n","\n","# extract negative sentences\n","test_neg_sent = test_data[test_label['ans'] == 'A']['OptionB']\n","test_neg_sent = test_neg_sent.append(test_data[test_label['ans'] == 'A']['OptionC'])\n","test_neg_sent = test_neg_sent.append(test_data[test_label['ans'] == 'B']['OptionA'])\n","test_neg_sent = test_neg_sent.append(test_data[test_label['ans'] == 'B']['OptionC'])\n","test_neg_sent = test_neg_sent.append(test_data[test_label['ans'] == 'C']['OptionA'])\n","test_neg_sent = test_neg_sent.append(test_data[test_label['ans'] == 'C']['OptionB'])\n","\n","#combining both positive and negative sentences\n","   # sensible sentences --> 0\n","   # nonsense sentences --> 1\n","testX = test_pos_sent.append(test_neg_sent)\n","# create labels for the sentences\n","testY = pd.Series([0]*2021).append(pd.Series([1]*4042))\n","\n","# reset index and avoid old index being added mistakenly as a column\n","testX = testX.reset_index(drop=True)\n","testY = testY.reset_index(drop=True)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EzVRPL_HeZ4e","colab_type":"text"},"source":["### **Remove non-values in both training and testing data**"]},{"cell_type":"code","metadata":{"id":"XfGLEVcVeh1N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595518568631,"user_tz":-60,"elapsed":9067,"user":{"displayName":"Xingyi Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNYWYUtCqJeuV1dkrvw6FkebdtDzj5ZkHCPlSTRA=s64","userId":"01867124944861915148"}}},"source":["#remove nan values in train data\n","for x in np.where(trainX.isnull())[0]:\n","    trainX = trainX.drop(index=x)\n","    trainY = trainY.drop(index=x)\n","\n","# remove nan values in test data\n","for y in np.where(testX.isnull())[0]:\n","    testX = testX.drop(index=y)\n","    testY = testY.drop(index=y)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mR1Z_yxodmVh","colab_type":"text"},"source":["### **Create tf-idf vectors for both training and testing data**"]},{"cell_type":"code","metadata":{"id":"JgUftdtqdoN3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595518571536,"user_tz":-60,"elapsed":11036,"user":{"displayName":"Xingyi Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNYWYUtCqJeuV1dkrvw6FkebdtDzj5ZkHCPlSTRA=s64","userId":"01867124944861915148"}}},"source":["# transfrom words into integers\n","vectorizer = TfidfVectorizer(max_features=2000, analyzer='word', ngram_range=(1, 4))\n","trainX_tfidf = vectorizer.fit_transform(trainX)\n","testX_tfidf = vectorizer.fit_transform(testX)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jTwvSZxwdxEX","colab_type":"text"},"source":["### **Performs classification to get accuracy score**"]},{"cell_type":"code","metadata":{"id":"_q2wEeeMdyq7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1595518806983,"user_tz":-60,"elapsed":243541,"user":{"displayName":"Xingyi Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjNYWYUtCqJeuV1dkrvw6FkebdtDzj5ZkHCPlSTRA=s64","userId":"01867124944861915148"}},"outputId":"4f69cdb6-c8f4-45a2-afad-7dad8ecf7154"},"source":["# naive bayes classifier\n","nb = MultinomialNB()\n","nb.fit(trainX_tfidf, trainY)\n","print('Accuracy for naive bayes:', nb.score(testX_tfidf, testY)*100)\n","\n","# k-nearest neighbour classifier     \n","knn = KNeighborsClassifier()\n","knn.fit(trainX_tfidf, trainY)\n","print('Accuracy for k-nn:', knn.score(testX_tfidf, testY)*100)\n","\n","# random forest classifier\n","rf = RandomForestClassifier()\n","rf.fit(trainX_tfidf, trainY)\n","print('Accuracy for random forest:', rf.score(testX_tfidf, testY)*100)\n","\n","# svm classifier\n","svm = SVC()\n","svm.fit(trainX_tfidf, trainY)\n","print('Accuracy svm:', svm.score(testX_tfidf, testY)*100)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Accuracy for naive bayes: 62.70207852193995\n","Accuracy for k-nn: 64.84658528538436\n","Accuracy for random forest: 60.722533817222036\n","Accuracy svm: 65.0940283734741\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nuTBMmedjOgJ","colab_type":"text"},"source":["### **Conclusion**"]},{"cell_type":"markdown","metadata":{"id":"R9ryRn5TjRd9","colab_type":"text"},"source":["For classification, 4 different type of classification methods had been used. They include Multinomial Naive Bayes Classifier, k-Nearest Neighbour Classifier, Random Forest Classifier and Support Vector Machine Classifier. After comparing the accuracy of these classifiers, it can be concluded that all these classifiers will give an accuracy score between 50%. \n","\n","Among all the 4 classifiers, Support Vector Machine classifier performs best with the accuracy of 65.09%."]}]}